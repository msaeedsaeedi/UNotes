## ğŸ”¹ **1.1 Random Variables (RVs)**

### ğŸ”¸ What is a Random Variable?

A **Random Variable (RV)** is a variable that takes on numerical values based on the outcome of a random experiment.

There are **two types**:

| Type            | Description                                                                 |
|-----------------|-----------------------------------------------------------------------------|
| **Discrete**    | Takes **countable** values (e.g., 0, 1, 2, â€¦).                              |
| **Continuous**  | Takes **uncountably infinite** values within an interval (e.g., 1.3, 2.7). |

---

### ğŸ”¹ Examples

#### Discrete RV:
- Tossing a coin 3 times â†’ Let $ X $ = number of heads.
  - Possible values of $ X $: {0, 1, 2, 3}
  
#### Continuous RV:
- Measuring the height of a person â†’ Let $ Y $ = height in cm.
  - Possible values of $ Y $: Any real number between, say, 100 and 200.

---

### ğŸ”¸ Notation

- Random Variable: usually written in **uppercase** (e.g., $ X, Y $)
- Realized value: written in **lowercase** (e.g., $ x, y $)

> Example: If $ X $ is a random variable for dice roll, then $ P(X = 5) $ is the probability that the outcome is 5.

---

## ğŸ”¹ Key Differences (Discrete vs Continuous)

| Feature                 | Discrete RV                         | Continuous RV                          |
|-------------------------|-------------------------------------|----------------------------------------|
| Values                  | Countable (finite or infinite)      | Uncountably infinite                   |
| Representation          | **Probability Mass Function (PMF)** | **Probability Density Function (PDF)** |
| Total Probability       | $ \sum P(X = x) = 1 $             | $ \int f(x) \, dx = 1 $              |
| Probability at a point  | Non-zero $ P(X = x) > 0 $         | Zero $ P(X = x) = 0 $                |
| Example                 | Number of heads in 3 coin tosses    | Weight of a randomly chosen apple      |

---

### ğŸ“Œ Important Realizations:

1. **Discrete RV**: We assign probability to each value.
   - $ P(X = x) $ is meaningful and non-zero.
2. **Continuous RV**: We assign probability over **intervals**, not single points.
   - $ P(X = x) = 0 $, but $ P(a \le X \le b) > 0 $

---

### ğŸ¯ Quick Practice

**Q1:** Identify if the following RVs are discrete or continuous:

a. The number of messages you get in a day  
b. The temperature at noon in Islamabad  
c. The time it takes to solve a math problem  
d. The number of students in a class

ğŸ‘‰ Answers:
- a: Discrete  
- b: Continuous  
- c: Continuous  
- d: Discrete

## ğŸ”¹ **1.2 Probability Mass Function (PMF)** â€“ For Discrete Random Variables

### ğŸ”¸ Definition:

A **Probability Mass Function (PMF)** gives the probability that a **discrete** random variable is equal to a particular value.

$$
P(X = x) = p(x)
$$

It must satisfy:
1. $ p(x) \geq 0 $ for all $ x $
2. $ \sum_{x} p(x) = 1 $

---

### ğŸ§  Example 1: Tossing a fair coin twice

Let $ X $ be the number of heads.

- Sample space: {HH, HT, TH, TT}
- Values of $ X $: {0, 1, 2}
- PMF:

| $ x $ | 0   | 1   | 2   |
|--------|-----|-----|-----|
| $ p(x) $ | 1/4 | 2/4 | 1/4 |

$$
\sum p(x) = \frac{1}{4} + \frac{2}{4} + \frac{1}{4} = 1
$$

âœ… This is a valid PMF.

---

### ğŸ§  Example 2: Rolling a fair die

Let $ X $ = outcome of the die.

- PMF: $ p(x) = \frac{1}{6} $ for $ x = 1, 2, 3, 4, 5, 6 $
- Not defined for other values.

---

## ğŸ”¹ **1.2 Probability Density Function (PDF)** â€“ For Continuous Random Variables

### ğŸ”¸ Definition:

A **Probability Density Function (PDF)** is a function that describes the **relative likelihood** of a **continuous** random variable to take on a value within a certain interval.

$$
f(x) \ge 0 \quad \text{and} \quad \int_{-\infty}^{\infty} f(x)\,dx = 1
$$

ğŸ‘‰ You **cannot** say $ P(X = x) $. Instead:

$$
P(a \le X \le b) = \int_a^b f(x) \, dx
$$

---

### ğŸ§  Example 1: Uniform Distribution over [0, 1]

$$
f(x) = \begin{cases}
1 & \text{if } 0 \le x \le 1 \\
0 & \text{otherwise}
\end{cases}
$$

Check:  
$$
\int_0^1 1 \, dx = 1 \Rightarrow \text{Valid PDF}
$$

Now:  
$$
P(0.2 \le X \le 0.5) = \int_{0.2}^{0.5} 1 \, dx = 0.5 - 0.2 = 0.3
$$

---

### ğŸ§  Example 2: Exponential Distribution (with $ \lambda = 1 $)

$$
f(x) = e^{-x}, \quad x \ge 0
$$

Check:  
$$
\int_0^\infty e^{-x} dx = 1
$$

This is a valid PDF.

---

## ğŸ“Š PMF vs PDF â€“ Summary Table

| Feature               | PMF (Discrete)            | PDF (Continuous)                        |
|----------------------|---------------------------|-----------------------------------------|
| Definition            | $ P(X = x) $            | $ f(x) $ such that $ P(a \le X \le b) = \int_a^b f(x)\,dx $ |
| Total Probability     | $ \sum p(x) = 1 $       | $ \int f(x) dx = 1 $                  |
| Probability at a Point| Positive value possible   | Always 0                                |
| Example               | Dice roll, coin toss      | Height, weight, time                    |

---

### ğŸ¯ Quick Check:

1. Is $ P(X = 1.5) $ meaningful for a **continuous** random variable?  
2. Can PMF ever be negative?  
3. What condition must a function satisfy to be a valid PDF?

ğŸ‘‰ Answers:
1. âŒ No, $ P(X = 1.5) = 0 $ for continuous RVs  
2. âŒ No, PMF $ \ge 0 $  
3. âœ… $ f(x) \ge 0 $, and total area under curve = 1

## ğŸ”¹ **1.3 Cumulative Distribution Function (CDF)**

### ğŸ”¸ Definition:

The **Cumulative Distribution Function (CDF)** of a random variable $ X $, denoted by $ F(x) $, gives the **probability that $ X $ takes a value less than or equal to $ x $:**

$$
F(x) = P(X \le x)
$$

> âœ”ï¸ This definition works for both **discrete** and **continuous** random variables.

---

## âœ… Properties of a CDF

| Property | Description |
|---------|-------------|
| **1.** $ 0 \le F(x) \le 1 $ | CDF is always between 0 and 1 |
| **2.** $ F(x) $ is **non-decreasing** | As $ x $ increases, $ F(x) $ does not decrease |
| **3.** $ \lim_{x \to -\infty} F(x) = 0 $ | Left tail â†’ probability = 0 |
| **4.** $ \lim_{x \to \infty} F(x) = 1 $ | Right tail â†’ total probability = 1 |
| **5.** For **continuous RV**, CDF is **continuous** |
| **6.** For **discrete RV**, CDF is **stepwise / jump function** |

---

## ğŸŸª For **Discrete Random Variables**

$$
F(x) = \sum_{t \le x} P(X = t)
$$

### ğŸ§  Example:

Toss 2 fair coins. Let $ X $ = number of heads. We already know:

| $ x $ | 0   | 1   | 2   |
|--------|-----|-----|-----|
| $ p(x) $ | 1/4 | 2/4 | 1/4 |

Compute CDF $ F(x) $:

| $ x $ | $ F(x) = P(X \le x) $ |
|--------|--------------------------|
| $ x < 0 $ | 0 |
| 0 | $ 1/4 $ |
| 1 | $ 1/4 + 2/4 = 3/4 $ |
| 2 | $ 1 $ |
| $ x > 2 $ | 1 |

Plot: The graph will look like a **staircase**.

---

## ğŸŸ¨ For **Continuous Random Variables**

$$
F(x) = \int_{-\infty}^{x} f(t) \, dt
$$

### ğŸ§  Example:

Let $ f(x) = 1 $, for $ 0 \le x \le 1 $. (Uniform Distribution)

$$
F(x) = \begin{cases}
0 & x < 0 \\
\int_0^x 1 dt = x & 0 \le x \le 1 \\
1 & x > 1
\end{cases}
$$

âœ… Check:
- $ F(0.4) = 0.4 $
- $ P(0.2 \le X \le 0.5) = F(0.5) - F(0.2) = 0.5 - 0.2 = 0.3 $

---

## ğŸ” Recover PMF/PDF from CDF

| Type       | Formula to recover          |
|------------|-----------------------------|
| **Discrete**  | $ P(X = x) = F(x) - F(x^-) $ *(jump height)* |
| **Continuous**| $ f(x) = \frac{d}{dx} F(x) $ *(derivative)*  |

---

## ğŸ¯ Practice Problems (Optional):

If you'd like, I can give you problems like:

> Given a stepwise CDF, determine the PMF.  
> Given a continuous CDF (e.g., $ F(x) = 1 - e^{-\lambda x} $), find the PDF and then calculate $ P(a < X < b) $.

Awesome! Let's now tackle a super important topic:

---

## âœ… **1.4: Expectation and Variance**

These are the **first and second moments** of a random variable â€” they measure **central tendency** and **spread**, respectively.

Your teacher may ask:
- Definitions for both discrete and continuous cases
- Full derivations
- Properties
- Applications in real-world scenarios
- Questions that combine expectation and variance with CDF/PMF/PDF

Letâ€™s break it down step by step ğŸ‘‡

---

## ğŸ”¹ 1.4.1 Expectation (Mean)

### ğŸ”¸ Definition:
The **expectation (mean)** of a random variable $ X $ is a **weighted average of all possible values** of $ X $.

---

### ğŸŸª For Discrete Random Variables:
$$
E[X] = \sum_x x \cdot p(x)
$$

Where:
- $ x $ = possible values of RV
- $ p(x) $ = PMF

---

### ğŸŸ¨ For Continuous Random Variables:
$$
E[X] = \int_{-\infty}^{\infty} x \cdot f(x) \, dx
$$

Where:
- $ f(x) $ = PDF

---

### ğŸ§  Example 1 (Discrete):
Let $ X $ be the outcome of rolling a fair die.

$$
E[X] = \sum_{x=1}^{6} x \cdot \frac{1}{6} = \frac{1+2+3+4+5+6}{6} = 3.5
$$

---

### ğŸ§  Example 2 (Continuous):
Let $ f(x) = 2x $, $ 0 \le x \le 1 $.  
(Valid PDF: check that $ \int_0^1 2x dx = 1 $)

Now:
$$
E[X] = \int_0^1 x \cdot 2x \, dx = \int_0^1 2x^2 dx = \left[\frac{2x^3}{3}\right]_0^1 = \frac{2}{3}
$$

---

## ğŸ”¹ 1.4.2 Variance

### ğŸ”¸ Definition:
Variance measures the **spread** of the random variable around the mean.

---

### Formula:
$$
\text{Var}(X) = E[(X - \mu)^2] = E[X^2] - (E[X])^2
$$

Where:
- $ \mu = E[X] $

---

### ğŸŸª For Discrete:
$$
\text{Var}(X) = \sum_x (x - \mu)^2 \cdot p(x) = \sum_x x^2 p(x) - (E[X])^2
$$

---

### ğŸŸ¨ For Continuous:
$$
\text{Var}(X) = \int_{-\infty}^{\infty} (x - \mu)^2 f(x) \, dx = \int x^2 f(x) dx - (E[X])^2
$$

---

### ğŸ§  Example (Rolling a fair die):

We already have $ E[X] = 3.5 $

$$
E[X^2] = \sum_{x=1}^{6} x^2 \cdot \frac{1}{6} = \frac{1^2 + 2^2 + \dots + 6^2}{6} = \frac{91}{6}
$$

$$
\text{Var}(X) = E[X^2] - (E[X])^2 = \frac{91}{6} - (3.5)^2 = \frac{91}{6} - \frac{49}{4} = \frac{364 - 294}{24} = \frac{70}{24} = 2.9167
$$

---

## ğŸ” Recap with a Table

| Concept   | Discrete Formula                | Continuous Formula                    |
|-----------|----------------------------------|----------------------------------------|
| $ E[X] $ | $ \sum x \cdot p(x) $          | $ \int x \cdot f(x) \, dx $          |
| $ E[X^2] $ | $ \sum x^2 \cdot p(x) $        | $ \int x^2 \cdot f(x) \, dx $        |
| $ \text{Var}(X) $ | $ E[X^2] - (E[X])^2 $        | $ E[X^2] - (E[X])^2 $                |
| $ \sigma $ (SD) | $ \sqrt{\text{Var}(X)} $ | $ \sqrt{\text{Var}(X)} $             |

---

## ğŸ¯ Common Pitfalls:
- Donâ€™t forget to square **only** after integrating or summing.
- Expectation is **linear**, but variance is **not**.
- Always validate the **domain** and **PDF/PMF sum or integral** first.

## âœ… **1.5: Properties of Expectation and Variance**

We'll cover:
- Linearity of Expectation
- Variance of a Linear Combination
- Scaling and Shifting Rules
- Expectations and Variances of Sums
- Independent variable properties

---

## ğŸ”· 1.5.1 Properties of Expectation

Expectation is **linear**, whether the variables are independent or not.

### ğŸ“Œ Property 1: **Linearity**
$$
E[aX + b] = aE[X] + b
$$
- $ a, b \in \mathbb{R} $ (constants)
- Holds for **both discrete and continuous** random variables

### ğŸ§  Example:
Let $ E[X] = 5 $

Then,
$$
E[3X + 2] = 3 \cdot 5 + 2 = 17
$$

---

### ğŸ“Œ Property 2: Expectation of a Sum
$$
E[X + Y] = E[X] + E[Y]
$$
- True **regardless of independence**

$$
E\left[\sum_{i=1}^n X_i\right] = \sum_{i=1}^n E[X_i]
$$

---

### ğŸ“Œ Property 3: Expectation of a Constant
$$
E[c] = c
$$
If $ X $ is always equal to $ c $, then its expected value is $ c $.

---

### ğŸ“Œ Property 4: Expectation of a Product (If Independent)
$$
E[XY] = E[X] \cdot E[Y] \quad \text{if } X, Y \text{ are independent}
$$

This does **not** hold if $ X, Y $ are dependent.

---

## ğŸ”¶ 1.5.2 Properties of Variance

Variance is **not** linear like expectation. Be careful!

---

### ğŸ“Œ Property 1: Variance of a Constant
$$
\text{Var}(c) = 0
$$

---

### ğŸ“Œ Property 2: Scaling Rule
$$
\text{Var}(aX) = a^2 \cdot \text{Var}(X)
$$

- Scaling affects variance **quadratically**

---

### ğŸ“Œ Property 3: Shifting Rule
$$
\text{Var}(X + c) = \text{Var}(X)
$$

- Adding a constant shifts the data but doesn't change the spread

---

### ğŸ“Œ Property 4: Variance of a Linear Combination
Let $ Y = aX + b $

$$
\text{Var}(aX + b) = a^2 \cdot \text{Var}(X)
$$

---

### ğŸ“Œ Property 5: Variance of Sum (Independent)
$$
\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) \quad \text{if } X, Y \text{ are independent}
$$

$$
\text{Var}\left(\sum_{i=1}^n X_i\right) = \sum_{i=1}^n \text{Var}(X_i) \quad \text{(for independent variables)}
$$

---

### ğŸ“Œ Property 6: If Not Independent
$$
\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y) + 2 \cdot \text{Cov}(X, Y)
$$

Weâ€™ll explore **Covariance** in more detail in joint distributions.

---

## ğŸ¯ Summary Table

| Operation                      | Expectation                      | Variance                          |
|-------------------------------|----------------------------------|-----------------------------------|
| $ E[aX + b] $               | $ aE[X] + b $                  | $ a^2 \text{Var}(X) $           |
| $ E[X + Y] $                | $ E[X] + E[Y] $                | $ \text{Var}(X) + \text{Var}(Y) $ *(if ind.)* |
| $ E[c] $                    | $ c $                          | $ 0 $                           |
| $ \text{Var}(X + c) $       | -                                | $ \text{Var}(X) $               |
| $ \text{Var}(X + Y) $       | -                                | $ \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y) $ |

---

## ğŸ§  Typical Exam Question:
> Let $ E[X] = 3, \text{Var}(X) = 4 $. Let $ Y = 5X + 2 $. Find $ E[Y] $, $ \text{Var}(Y) $.

**Solution**:
- $ E[Y] = 5E[X] + 2 = 5(3) + 2 = 17 $
- $ \text{Var}(Y) = 25 \cdot \text{Var}(X) = 25 \cdot 4 = 100 $

## âœ… **1.6: Moments (About the Mean and Origin)**

Moments describe the **shape** and **features** of a probability distribution:
- **Central moments** focus on spread around the **mean**
- **Raw moments** are about spread around the **origin (0)**

---

## ğŸ”¹ 1.6.1 What is a Moment?

The $ r $-th moment of a random variable $ X $ is defined as:

### ğŸ”¸ About the Origin:
$$
\mu'_r = E[X^r]
$$

### ğŸ”¸ About the Mean (Central Moment):
$$
\mu_r = E[(X - \mu)^r]
$$
Where $ \mu = E[X] $

---

## ğŸ”¸ Interpretation of First Few Moments:

| Moment | About Origin            | About Mean (Central)             | Interpretation                  |
|--------|--------------------------|----------------------------------|---------------------------------|
| 1st    | $ E[X] $              | $ 0 $                          | Mean (location of distribution) |
| 2nd    | $ E[X^2] $            | $ \text{Var}(X) $              | Spread (variance)               |
| 3rd    | $ E[X^3] $            | $ E[(X - \mu)^3] $             | Skewness (symmetry)             |
| 4th    | $ E[X^4] $            | $ E[(X - \mu)^4] $             | Kurtosis (peakedness)           |

---

## ğŸ”¹ 1.6.2 Raw Moments (About Origin)

$$
\mu'_1 = E[X], \quad \mu'_2 = E[X^2], \quad \mu'_3 = E[X^3], \quad \mu'_4 = E[X^4]
$$

These are easier to compute directly from PMF/PDF.

---

### ğŸ§  Example (Discrete):
Let $ X $ have values $ 1, 2, 3 $ with probabilities $ \frac{1}{6}, \frac{1}{2}, \frac{1}{3} $

**Step 1: First raw moment**
$$
\mu'_1 = E[X] = 1 \cdot \frac{1}{6} + 2 \cdot \frac{1}{2} + 3 \cdot \frac{1}{3} = \frac{1}{6} + 1 + 1 = \frac{13}{6}
$$

**Step 2: Second raw moment**
$$
\mu'_2 = E[X^2] = 1^2 \cdot \frac{1}{6} + 2^2 \cdot \frac{1}{2} + 3^2 \cdot \frac{1}{3} = \frac{1}{6} + 2 + 3 = \frac{31}{6}
$$

---

## ğŸ”¹ 1.6.3 Central Moments (About Mean)

$$
\mu_2 = E[(X - \mu)^2] = \text{Var}(X)
$$

$$
\mu_3 = E[(X - \mu)^3] \quad \text{(Skewness)}
$$

$$
\mu_4 = E[(X - \mu)^4] \quad \text{(Kurtosis)}
$$

---

### ğŸ§  Example (Using Earlier Data):

Letâ€™s reuse:
- $ \mu = \mu'_1 = \frac{13}{6} $
- $ \mu'_2 = \frac{31}{6} $

Now compute:
$$
\mu_2 = \text{Var}(X) = E[X^2] - (E[X])^2 = \frac{31}{6} - \left(\frac{13}{6}\right)^2 = \frac{31}{6} - \frac{169}{36}
$$

To subtract:
$$
\frac{31}{6} = \frac{186}{36}, \quad \Rightarrow \mu_2 = \frac{186 - 169}{36} = \frac{17}{36}
$$

---

## ğŸ¯ Summary

| Moment      | Formula                              | Use                         |
|-------------|---------------------------------------|------------------------------|
| Raw moment  | $ \mu'_r = E[X^r] $                | Simpler to compute           |
| Central moment | $ \mu_r = E[(X - \mu)^r] $         | Describes distribution shape |
| Variance    | $ \mu_2 $                          | Spread                       |
| Skewness    | $ \mu_3 $                          | Asymmetry                    |
| Kurtosis    | $ \mu_4 $                          | Peakedness                   |

## âœ… **1.7: Moment Generating Functions (MGF)**

MGFs are not just theoreticalâ€”theyâ€™re used to:
- Derive **all moments** of a distribution
- Identify distributions (like a fingerprint)
- Prove convergence in probability theory

Letâ€™s understand this step-by-step:

---

## ğŸ”¹ 1.7.1 Definition

For a random variable $ X $, the **Moment Generating Function (MGF)** is:

$$
M_X(t) = E[e^{tX}]
$$

> If the MGF exists in a neighborhood around $ t = 0 $, then:
$$
\text{The } r\text{-th moment of } X = M_X^{(r)}(0) = \left.\frac{d^r}{dt^r} M_X(t)\right|_{t=0}
$$

---

## ğŸ”¸ 1.7.2 Why is it Called â€œMoment Generatingâ€?

MGF **generates moments** by differentiating:
- First derivative gives **first moment** (mean)
- Second derivative gives **second raw moment**
- And so on

---

### ğŸ”¸ Properties of MGF:

| Property | Description |
|----------|-------------|
| $ M_X(0) = 1 $ | Always holds |
| $ M_{aX + b}(t) = e^{bt} \cdot M_X(at) $ | Linear transformation |
| If $ X $ and $ Y $ are independent â†’ $ M_{X+Y}(t) = M_X(t) \cdot M_Y(t) $ | Used for sums |

---

## ğŸ”¹ 1.7.3 Step-by-Step Example (Discrete)

Letâ€™s find MGF of a discrete RV:

### ğŸ² Example: Let $ X $ take values $ 0, 1 $ with $ P(X=0)=0.4, \ P(X=1)=0.6 $

Step 1: Write the definition:
$$
M_X(t) = E[e^{tX}] = \sum_x e^{tx} \cdot P(X = x)
$$

Step 2: Plug values:
$$
M_X(t) = e^{t \cdot 0} \cdot 0.4 + e^{t \cdot 1} \cdot 0.6 = 0.4 + 0.6e^t
$$

---

### âœ… Mean from MGF:
Differentiate and plug $ t = 0 $

$$
M'_X(t) = 0.6e^t \quad \Rightarrow M'_X(0) = 0.6
\Rightarrow E[X] = 0.6
$$

---

### âœ… Second Moment:
$$
M''_X(t) = 0.6e^t \Rightarrow M''_X(0) = 0.6
\Rightarrow E[X^2] = 0.6
$$

### âœ… Variance:
$$
\text{Var}(X) = E[X^2] - (E[X])^2 = 0.6 - (0.6)^2 = 0.6 - 0.36 = 0.24
$$

---

## ğŸ”¹ 1.7.4 Step-by-Step Example (Continuous)

### ğŸ” Let $ X \sim \text{Exponential}(\lambda) $, PDF:  
$$
f(x) = \lambda e^{-\lambda x}, \quad x \geq 0
$$

Find MGF:
$$
M_X(t) = E[e^{tX}] = \int_0^\infty e^{tx} \cdot \lambda e^{-\lambda x} \, dx
= \lambda \int_0^\infty e^{(t - \lambda)x} \, dx
$$

This converges **only when $ t < \lambda $**:
$$
M_X(t) = \frac{\lambda}{\lambda - t}
$$

From here:
- $ M'_X(0) = \frac{\lambda}{(\lambda - 0)^2} = \frac{1}{\lambda} $
- So, $ E[X] = \frac{1}{\lambda} $
- And $ \text{Var}(X) = \frac{1}{\lambda^2} $

---

## ğŸ¯ Summary

| Feature | Formula / Insight |
|--------|-------------------|
| Definition | $ M_X(t) = E[e^{tX}] $ |
| 1st moment | $ M'_X(0) = E[X] $ |
| 2nd moment | $ M''_X(0) = E[X^2] $ |
| Variance | $ M''_X(0) - (M'_X(0))^2 $ |
| MGF of sum | $ M_{X+Y}(t) = M_X(t) \cdot M_Y(t) $ (if independent) |

---

## ğŸ§  Tips:
- **Always check convergence range** of MGF.
- When asked about MGF in exams, **differentiate clearly** and show evaluation at $ t = 0 $.
- For known distributions, memorize MGFs for speed (Bernoulli, Binomial, Poisson, etc.)

## âœ… **1.8: Cumulant Generating Functions (CGF)**

Cumulants are an alternative to moments and give **deeper insights** into the distribution's structureâ€”like **skewness**, **kurtosis**, etc.

---

## ğŸ”¹ 1.8.1 What is the Cumulant Generating Function?

The **Cumulant Generating Function (CGF)** is the natural logarithm of the MGF:

$$
K_X(t) = \ln M_X(t)
$$

- If the MGF exists in a neighborhood around $ t = 0 $, then the CGF is also defined.
- Cumulants are generated by **differentiating the CGF**, just like how moments are from the MGF.

---

## ğŸ”¹ 1.8.2 First Few Cumulants

$$
\kappa_1 = K'_X(0) = \text{Mean}
$$  
$$
\kappa_2 = K''_X(0) = \text{Variance}
$$  
$$
\kappa_3 = K^{(3)}_X(0) = \text{Measure of Skewness (shape)}
$$  
$$
\kappa_4 = K^{(4)}_X(0) = \text{Related to Kurtosis}
$$

> So, **first cumulant = mean**, **second = variance**, and **third & fourth** describe **shape** of the distribution.

---

## ğŸ”¹ 1.8.3 Why Do We Use CGF?

| Purpose | Explanation |
|--------|-------------|
| ğŸ§  Simpler Algebra | Log of products = sum of logs, so easier with sums of RVs |
| â• Additive Property | Cumulants of sum = sum of cumulants |
| ğŸ¯ Shape Insights | Higher-order cumulants describe distribution shape (like skewness & kurtosis) |
| ğŸ’¡ Less Sensitive | Cumulants are often more stable than moments in extreme values |

---

## ğŸ” Example: Use CGF to Find Mean and Variance

Letâ€™s say you already found an MGF:

$$
M_X(t) = 0.4 + 0.6e^t
$$

So the CGF is:

$$
K_X(t) = \ln(0.4 + 0.6e^t)
$$

To find cumulants:

### Step 1: Differentiate once (mean)
$$
K'_X(t) = \frac{d}{dt} \ln(0.4 + 0.6e^t) = \frac{0.6e^t}{0.4 + 0.6e^t}
$$
$$
K'_X(0) = \frac{0.6}{0.4 + 0.6} = 0.6 \Rightarrow \kappa_1 = \text{Mean} = 0.6
$$

---

### Step 2: Differentiate again (variance)
Apply quotient rule:
$$
K''_X(t) = \frac{(0.6e^t)(0.4 + 0.6e^t) - (0.6e^t)^2}{(0.4 + 0.6e^t)^2}
$$

At $ t = 0 $:
$$
K''_X(0) = \frac{0.6(0.4 + 0.6) - 0.36}{(1)^2} = 0.6 - 0.36 = 0.24
\Rightarrow \kappa_2 = \text{Variance}
$$

---

## ğŸ¯ Summary Table

| Cumulant | Formula | Meaning |
|----------|---------|---------|
| $ \kappa_1 $ | $ K'_X(0) $ | Mean |
| $ \kappa_2 $ | $ K''_X(0) $ | Variance |
| $ \kappa_3 $ | $ K^{(3)}_X(0) $ | Skewness indicator |
| $ \kappa_4 $ | $ K^{(4)}_X(0) $ | Kurtosis indicator |

---

## ğŸ§  Tips:
- **Moments and cumulants** match for the first two (mean and variance).
- Cumulants are **additive for independent RVs**, so theyâ€™re useful in sum-based problems.
- You **donâ€™t need to memorize higher derivatives**â€”focus on how to use CGF for mean/variance.

## âœ… **1.9: Measures of Central Tendency for Random Variables**

Weâ€™ve already talked about **expectation (mean)** as a measure of central tendency. Now, weâ€™ll cover:

- Median  
- Mode  
- Quartiles  

These are often overlooked for random variables, but **your teacher might ask** them in a tricky wayâ€”so weâ€™ll go **step-by-step**.

---

## ğŸ”¹ 1.9.1 Mean (Quick Recap)

| Type | Formula |
|------|---------|
| Discrete | $ \mu = E[X] = \sum x \cdot P(X=x) $ |
| Continuous | $ \mu = E[X] = \int x \cdot f(x) \, dx $ |

Mean is the **expected center**, but itâ€™s sensitive to **extreme values**.

---

## ğŸ”¹ 1.9.2 Median of a Random Variable

**Definition:** The median is the value $ m $ such that:

$$
P(X \leq m) \geq 0.5 \quad \text{and} \quad P(X \geq m) \geq 0.5
$$

> Median is the **middle** of the distribution and **less sensitive** to outliers than the mean.

---

### ğŸ”¸ Example (Discrete)

Let $ X $ have this PMF:

| $ X $ | 1 | 2 | 3 | 4 | 5 |
|--------|---|---|---|---|---|
| $ P(X=x) $ | 0.1 | 0.2 | 0.4 | 0.2 | 0.1 |

Compute CDF:

| $ x $ | CDF $ P(X \leq x) $ |
|--------|-----------------------|
| 1 | 0.1 |
| 2 | 0.3 |
| 3 | 0.7 |
| 4 | 0.9 |
| 5 | 1.0 |

â¡ï¸ Median is the **first value where CDF â‰¥ 0.5**, so median = **3**

---

### ğŸ”¸ Example (Continuous)

Let $ X \sim \text{Uniform}(0, 2) $

Then PDF:
$$
f(x) = \frac{1}{2}, \quad 0 \leq x \leq 2
$$

To find median:

$$
P(X \leq m) = \int_0^m \frac{1}{2} dx = \frac{m}{2} = 0.5
\Rightarrow m = 1
$$

â¡ï¸ Median = **1**

---

## ğŸ”¹ 1.9.3 Mode of a Random Variable

**Definition:** The mode is the value $ x $ at which the **PMF or PDF** is maximized.

---

### ğŸ”¸ Discrete Example

Same PMF:

| $ X $ | 1 | 2 | 3 | 4 | 5 |
|--------|---|---|---|---|---|
| $ P(X=x) $ | 0.1 | 0.2 | 0.4 | 0.2 | 0.1 |

Here, max probability is at $ x = 3 $

â¡ï¸ Mode = **3**

---

### ğŸ”¸ Continuous Example

Let $ X \sim \text{Exponential}(\lambda) $, PDF:

$$
f(x) = \lambda e^{-\lambda x}, \quad x \geq 0
$$

This function is **maximum at $ x = 0 $**

â¡ï¸ Mode = **0**

---

## ğŸ”¹ 1.9.4 Quartiles of a Random Variable

Quartiles split the distribution into four parts.

- $ Q_1 $: First quartile (25% of values â‰¤ this)
- $ Q_2 $: Median (50%)
- $ Q_3 $: Third quartile (75%)

---

### ğŸ”¸ Discrete Example

From earlier:

| $ x $ | 1 | 2 | 3 | 4 | 5 |
|--------|---|---|---|---|---|
| $ P(X=x) $ | 0.1 | 0.2 | 0.4 | 0.2 | 0.1 |
| CDF | 0.1 | 0.3 | 0.7 | 0.9 | 1.0 |

- $ Q_1 $: First value with CDF â‰¥ 0.25 â†’ **2**
- $ Q_2 $: Median â†’ **3**
- $ Q_3 $: First value with CDF â‰¥ 0.75 â†’ **4**

---

### ğŸ”¸ Continuous Example: Uniform(0, 2)

PDF: $ f(x) = \frac{1}{2} $

To find $ Q_1 $:

$$
P(X \leq Q_1) = 0.25 = \int_0^{Q_1} \frac{1}{2} dx = \frac{Q_1}{2}
\Rightarrow Q_1 = 0.5
$$

Similarly:

- $ Q_2 = 1 $ (median)
- $ Q_3 = 1.5 $

---

## ğŸ¯ Summary Table

| Measure | Meaning | How to Find |
|--------|---------|-------------|
| **Mean** | Average | $ E[X] $ |
| **Median** | Middle | Solve $ P(X \leq m) = 0.5 $ |
| **Mode** | Most probable | Max of PMF/PDF |
| **Quartiles** | 25%, 50%, 75% points | Solve $ P(X \leq Q_i) = i/4 $ |

---

## ğŸ§  Tips for Exams:
- Use **CDFs** to find medians and quartiles (discrete or continuous).
- If the PMF is **symmetrical**, mean = median = mode.
- Quartiles are especially useful when asked for **IQR (Interquartile Range) = Q3 - Q1**.

---

That completes **Part 1** of your syllabus in full detail! ğŸ‰  
We covered:

- Discrete & Continuous RVs  
- PMF/PDF, CDF  
- Expectation, Variance  
- Properties  
- Moments  
- MGF & CGF  
- Median, Mode, Quartiles  

## âœ… **2.1: Joint Distributions of Discrete and Continuous Random Variables**

When studying multiple random variables together (like $ X $ and $ Y $), we use **joint distributions** to understand how they behave together.

---

## ğŸ”· 2.1.1 Joint Probability Mass Function (Joint PMF) â€” *Discrete Case*

If $ X $ and $ Y $ are discrete random variables, their **joint PMF** is:

$$
P(X = x, Y = y) = \text{Probability that } X = x \text{ and } Y = y
$$

It satisfies:
1. $ P(X = x_i, Y = y_j) \geq 0 $
2. $ \sum_i \sum_j P(X = x_i, Y = y_j) = 1 $

---

### ğŸ”¸ Example (Joint PMF Table)

Letâ€™s say $ X \in \{1, 2\}, Y \in \{0, 1\} $, and:

| $ P(X = x, Y = y) $ | $ Y = 0 $ | $ Y = 1 $ |
|------------------------|------------|------------|
| $ X = 1 $           | 0.2        | 0.3        |
| $ X = 2 $           | 0.1        | 0.4        |

Check:
$$
0.2 + 0.3 + 0.1 + 0.4 = 1 âœ…
$$

---

## ğŸ”· 2.1.2 Joint Probability Density Function (Joint PDF) â€” *Continuous Case*

If $ X $ and $ Y $ are continuous random variables, then their **joint PDF** is a function $ f(x, y) $ such that:

$$
P((X, Y) \in A) = \iint_A f(x, y) \, dx\,dy
$$

It must satisfy:

1. $ f(x, y) \geq 0 $
2. $ \iint_{-\infty}^\infty f(x, y)\,dx\,dy = 1 $

---

### ğŸ”¸ Example (Joint PDF)

Let:

$$
f(x, y) = 
\begin{cases}
4xy, & 0 \leq x \leq 1,\, 0 \leq y \leq 1 \\
0, & \text{otherwise}
\end{cases}
$$

Letâ€™s verify itâ€™s a valid joint PDF:

$$
\iint_0^1 \int_0^1 4xy\,dx\,dy = \int_0^1 \left[2x^2 y\right]_0^1 dy = \int_0^1 2y\,dy = \left[y^2\right]_0^1 = 1 âœ…
$$

---

## ğŸ§  Real-Life Interpretation

Letâ€™s say:
- $ X $ = time someone studies per day
- $ Y $ = number of assignments completed

A joint distribution shows how **time and performance** relate.

For instance:
- $ P(X = 2, Y = 4) = 0.1 $ means there's a 10% chance the student studies 2 hours and completes 4 assignments.

---

## ğŸ¯ Summary Table

| Concept | Discrete | Continuous |
|--------|----------|------------|
| Joint Function | $ P(X = x, Y = y) $ | $ f(x, y) $ |
| Total Sum | $ \sum_x \sum_y P(x, y) = 1 $ | $ \iint f(x, y)\,dx\,dy = 1 $ |
| Probability in Region | Sum of entries | Double integral over region |

## âœ… **2.2: Marginal Distributions**

**Marginal distribution** gives us the probability distribution of a single random variable *ignoring* the other one.

Basically:  
> From the **joint distribution**, we "sum out" or "integrate out" the other variable.

---

## ğŸ”· 2.2.1 Marginal Distribution (Discrete Case)

Given the **joint PMF** $ P(X = x_i, Y = y_j) $, the **marginal PMF** of $ X $ is:

$$
P_X(x_i) = \sum_j P(X = x_i, Y = y_j)
$$

Similarly, the **marginal PMF** of $ Y $ is:

$$
P_Y(y_j) = \sum_i P(X = x_i, Y = y_j)
$$

---

### ğŸ”¸ Example (From a Joint PMF Table)

| $ P(X = x, Y = y) $ | $ Y = 0 $ | $ Y = 1 $ | Marginal $ P_X(x) $ |
|------------------------|------------|------------|------------------------|
| $ X = 1 $           | 0.2        | 0.3        | 0.2 + 0.3 = **0.5**    |
| $ X = 2 $           | 0.1        | 0.4        | 0.1 + 0.4 = **0.5**    |
| Marginal $ P_Y(y) $ | 0.3        | 0.7        |                        |

$$
\text{Check: } \sum P_X(x) = 0.5 + 0.5 = 1 âœ…,\quad \sum P_Y(y) = 0.3 + 0.7 = 1 âœ…
$$

---

## ğŸ”· 2.2.2 Marginal Distribution (Continuous Case)

Given a **joint PDF** $ f(x, y) $, the **marginal PDF** of $ X $ is:

$$
f_X(x) = \int_{-\infty}^{\infty} f(x, y) \, dy
$$

And for $ Y $:

$$
f_Y(y) = \int_{-\infty}^{\infty} f(x, y) \, dx
$$

---

### ğŸ”¸ Example

Let:
$$
f(x, y) = 4xy, \quad 0 \leq x \leq 1,\ 0 \leq y \leq 1
$$

Find the marginal PDF of $ X $:

$$
f_X(x) = \int_0^1 4xy \, dy = 4x \int_0^1 y \, dy = 4x \cdot \left[ \frac{y^2}{2} \right]_0^1 = 4x \cdot \frac{1}{2} = 2x
$$

So,
$$
f_X(x) = 
\begin{cases}
2x, & 0 \leq x \leq 1 \\
0, & \text{otherwise}
\end{cases}
$$

Similarly:
$$
f_Y(y) = \int_0^1 4xy \, dx = 4y \int_0^1 x \, dx = 4y \cdot \frac{1}{2} = 2y
$$

---

## ğŸ§  Real-Life Analogy

Suppose you record data about:
- $ X $ = number of hours studied
- $ Y $ = number of assignments submitted

If you only care about **how many hours were studied**, no matter how many assignments were done, you'd compute the **marginal distribution of $ X $** by **summing over** $ Y $.

---

## âœ… Quick Summary

| Type | Joint | Marginal of X | Marginal of Y |
|------|-------|----------------|----------------|
| Discrete | $ P(x, y) $ | $ \sum_y P(x, y) $ | $ \sum_x P(x, y) $ |
| Continuous | $ f(x, y) $ | $ \int f(x, y)\,dy $ | $ \int f(x, y)\,dx $ |

## âœ… **2.3: Conditional Distributions**

Conditional distributions tell us how the probability of one variable behaves *under the condition* that we know the value of the other variable.

---

## ğŸ”· 2.3.1 Discrete Case â€” Conditional PMF

Let $ X $ and $ Y $ be discrete random variables with a known **joint PMF** $ P(X = x, Y = y) $.

Then the **conditional probability** of $ X = x $ given $ Y = y $ is:

$$
P(X = x \mid Y = y) = \frac{P(X = x, Y = y)}{P_Y(y)} \quad \text{(if } P_Y(y) > 0 \text{)}
$$

Likewise:
$$
P(Y = y \mid X = x) = \frac{P(X = x, Y = y)}{P_X(x)}
$$

---

### ğŸ”¸ Example

Using the same table from earlier:

| $ P(X = x, Y = y) $ | $ Y = 0 $ | $ Y = 1 $ | $ P_X(x) $ |
|------------------------|------------|------------|---------------|
| $ X = 1 $           | 0.2        | 0.3        | 0.5           |
| $ X = 2 $           | 0.1        | 0.4        | 0.5           |
| $ P_Y(y) $          | 0.3        | 0.7        |               |

Now:
- $ P(X = 1 \mid Y = 1) = \frac{0.3}{0.7} = \frac{3}{7} $
- $ P(Y = 0 \mid X = 2) = \frac{0.1}{0.5} = \frac{1}{5} $

âœ… Always check the denominator is not zero.

---

## ğŸ”· 2.3.2 Continuous Case â€” Conditional PDF

Let $ f(x, y) $ be a joint PDF. Then:

$$
f_{X \mid Y}(x \mid y) = \frac{f(x, y)}{f_Y(y)} \quad \text{(if } f_Y(y) > 0 \text{)}
$$

Similarly:

$$
f_{Y \mid X}(y \mid x) = \frac{f(x, y)}{f_X(x)}
$$

---

### ğŸ”¸ Example

Given:

$$
f(x, y) = 4xy,\quad 0 \le x, y \le 1
$$
Previously we found:
- $ f_X(x) = 2x $
- $ f_Y(y) = 2y $

Now:

$$
f_{X \mid Y}(x \mid y) = \frac{f(x, y)}{f_Y(y)} = \frac{4xy}{2y} = 2x
\quad \text{(as long as } y > 0\text{)}
$$

$$
f_{Y \mid X}(y \mid x) = \frac{4xy}{2x} = 2y
\quad \text{(as long as } x > 0\text{)}
$$

ğŸ’¡ Notice: The conditional PDFs are the same as marginals in this case â€” this hints at independence (weâ€™ll get to that next!).

---

## ğŸ¯ Interpretation in Real Life

Imagine you want to know:
> â€œWhat is the distribution of **hours studied (X)** given that a student submitted **4 assignments (Y = 4)**?â€

Youâ€™d use the **conditional distribution** of $ X \mid Y = 4 $.

This helps us **narrow down** the behavior of one variable when another is fixed.

---

## ğŸ” Comparison Summary

| Type | Conditional Formula |
|------|---------------------|
| Discrete | $ P(X = x \mid Y = y) = \frac{P(X = x, Y = y)}{P_Y(y)} $ |
| Continuous | $ f_{X \mid Y}(x \mid y) = \frac{f(x, y)}{f_Y(y)} $ |

## âœ… **2.4: Independence of Random Variables**

Two random variables are **independent** if *knowing the value of one gives no information about the other*.  
In other words:  
> The behavior of one does **not depend** on the other.

---

## ğŸ”· 2.4.1 Independence â€” Discrete Case

Let $ X $ and $ Y $ be discrete random variables.

### ğŸ‘‰ Definition:

$ X $ and $ Y $ are **independent** if and only if:

$$
P(X = x, Y = y) = P_X(x) \cdot P_Y(y) \quad \text{for all } x, y
$$

That is, the **joint PMF** equals the **product of the marginals**.

---

### ğŸ”¸ Example:

From earlier:

| $ P(X = x, Y = y) $ | $ Y = 0 $ | $ Y = 1 $ | $ P_X(x) $ |
|------------------------|------------|------------|--------------|
| $ X = 1 $           | 0.2        | 0.3        | 0.5          |
| $ X = 2 $           | 0.1        | 0.4        | 0.5          |
| $ P_Y(y) $          | 0.3        | 0.7        |              |

Check:  
$ P(X = 1, Y = 0) = 0.2 $  
$ P_X(1) \cdot P_Y(0) = 0.5 \cdot 0.3 = 0.15 \ne 0.2 $

âŒ So, **not independent**

---

## ğŸ”· 2.4.2 Independence â€” Continuous Case

Let $ X $ and $ Y $ be continuous random variables.

### ğŸ‘‰ Definition:

$ X $ and $ Y $ are **independent** if and only if:

$$
f(x, y) = f_X(x) \cdot f_Y(y) \quad \text{for all } x, y
$$

So again, the **joint PDF** must be the **product of marginals**.

---

### ğŸ”¸ Example:

Let:
$$
f(x, y) = 4xy,\quad 0 \le x, y \le 1
$$

From earlier:
- $ f_X(x) = 2x $
- $ f_Y(y) = 2y $

Now:
$$
f(x, y) = 4xy = 2x \cdot 2y = f_X(x) \cdot f_Y(y)
$$

âœ… So, **independent**

---

## ğŸ” Shortcut: Conditional Distribution Test

Another way to check independence:

$$
P(X = x \mid Y = y) = P_X(x) \quad \text{(for discrete)}
$$
$$
f_{X \mid Y}(x \mid y) = f_X(x) \quad \text{(for continuous)}
$$

If the **conditional distribution equals the marginal**, the variables are **independent**.

---

## ğŸ¯ Real-Life Example

Let:
- $ X $ = number of hours a student sleeps
- $ Y $ = marks in a math quiz

If we find that knowing how much they sleep **does not change** the probability distribution of their marks, then $ X $ and $ Y $ are **independent**.

But usually, there **is** some dependence in real life!

---

## âœ… Summary Table

| Type | Independence Condition |
|------|------------------------|
| Discrete | $ P(X = x, Y = y) = P_X(x) \cdot P_Y(y) $ |
| Continuous | $ f(x, y) = f_X(x) \cdot f_Y(y) $ |

## âœ… **2.5: Variance and Covariance**

---

## ğŸ”· 2.5.1 Variance Recap (for a Single RV)

For a single random variable $ X $, we define:

$$
\operatorname{Var}(X) = E[(X - \mu_X)^2] = E[X^2] - (E[X])^2
$$

It tells us **how much the values of X deviate from the mean**.

---

## ğŸ”· 2.5.2 Covariance â€” Measuring Linear Relationship

Covariance tells us **how two random variables vary together**.

$$
\operatorname{Cov}(X, Y) = E[(X - \mu_X)(Y - \mu_Y)]
$$

Alternate formula:

$$
\operatorname{Cov}(X, Y) = E[XY] - E[X]E[Y]
$$

---

### ğŸŸ© Interpretation:

- **Positive Covariance**: When $ X $ increases, $ Y $ tends to increase.
- **Negative Covariance**: When $ X $ increases, $ Y $ tends to decrease.
- **Zero Covariance**: No linear relationship.

> Covariance gives the **direction** of the relationship, but **not strength or scale**.

---

### ğŸ”¸ Example (Discrete Case)

| $ (X, Y) $ | $ P(X, Y) $ |
|-------------|---------------|
| (1, 1)       | 0.25          |
| (1, 3)       | 0.25          |
| (3, 1)       | 0.25          |
| (3, 3)       | 0.25          |

Find:
- $ E[X] = 0.25(1 + 1 + 3 + 3) = 2 $
- $ E[Y] = 0.25(1 + 3 + 1 + 3) = 2 $
- $ E[XY] = 0.25(1 + 3 + 3 + 9) = 4 $

Then:
$$
\operatorname{Cov}(X, Y) = E[XY] - E[X]E[Y] = 4 - (2)(2) = 0
$$

âœ… So X and Y are **uncorrelated**.

---

### ğŸ”¸ Example (Continuous Case)

Let:
$$
f(x, y) = 4xy,\quad 0 \le x, y \le 1
$$

Weâ€™ve already found:
- $ E[X] = \frac{2}{3} $, $ E[Y] = \frac{2}{3} $

Now:
$$
E[XY] = \int_0^1 \int_0^1 x y \cdot 4xy \, dx\,dy = \int_0^1 \int_0^1 4x^2y^2 \, dx\,dy
$$

Compute:
- $ \int_0^1 x^2 dx = \frac{1}{3} $, $ \int_0^1 y^2 dy = \frac{1}{3} $
- So $ E[XY] = 4 \cdot \frac{1}{3} \cdot \frac{1}{3} = \frac{4}{9} $

Now:
$$
\operatorname{Cov}(X, Y) = E[XY] - E[X]E[Y] = \frac{4}{9} - \frac{4}{9} = 0
$$

Again, **uncorrelated**.

---

## ğŸ”· 2.5.3 Properties of Covariance

1. **Symmetry**:  
   $$
   \operatorname{Cov}(X, Y) = \operatorname{Cov}(Y, X)
   $$

2. **Cov(X, X) = Var(X)**

3. **Linearity**:  
   For constants $ a, b, c, d $:

   $$
   \operatorname{Cov}(aX + b, cY + d) = ac \cdot \operatorname{Cov}(X, Y)
   $$

---

## ğŸ”· 2.5.4 Variance of a Sum

$$
\operatorname{Var}(X + Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) + 2 \cdot \operatorname{Cov}(X, Y)
$$

âš ï¸ Use this when computing variance of totals (e.g., total score of 2 subjects).

---

## ğŸ” Summary

| Concept       | Formula |
|---------------|---------|
| $ \operatorname{Var}(X) $ | $ E[X^2] - (E[X])^2 $ |
| $ \operatorname{Cov}(X, Y) $ | $ E[XY] - E[X]E[Y] $ |
| $ \operatorname{Var}(X+Y) $ | $ \operatorname{Var}(X) + \operatorname{Var}(Y) + 2\operatorname{Cov}(X, Y) $ |

## âœ… **2.6: Correlation Coefficient**

---

## ğŸ”· 2.6.1 What is the Correlation Coefficient?

The **correlation coefficient**, denoted by $ \rho_{X,Y} $ (rho), **standardizes covariance**.

$$
\rho_{X,Y} = \frac{\operatorname{Cov}(X, Y)}{\sigma_X \cdot \sigma_Y}
$$

Where:
- $ \sigma_X = \sqrt{\operatorname{Var}(X)} $
- $ \sigma_Y = \sqrt{\operatorname{Var}(Y)} $

This tells us **how strongly X and Y are linearly related**, and the value is **always between -1 and 1**.

---

## ğŸ”· 2.6.2 Interpretation

| Value of $ \rho $ | Strength of Relationship |
|---------------------|--------------------------|
| $ \rho = 1 $      | Perfect positive linear relationship |
| $ \rho = -1 $     | Perfect negative linear relationship |
| $ \rho = 0 $      | No linear relationship (could still be non-linear) |
| $ \rho \approx 0.5 $ | Moderate positive linear relationship |
| $ \rho \approx -0.5 $ | Moderate negative linear relationship |

> **Key Point**: Correlation only captures **linear** dependence. Two variables may have **nonlinear dependence** and still have $ \rho = 0 $!

---

## ğŸ”¸ Example 1 (Discrete)

From previous example:

| (X, Y) | P(X, Y) |
|--------|---------|
| (1, 1) | 0.25    |
| (1, 3) | 0.25    |
| (3, 1) | 0.25    |
| (3, 3) | 0.25    |

We already found:

- $ E[X] = 2 $, $ E[Y] = 2 $
- $ E[XY] = 4 $
- $ \operatorname{Cov}(X, Y) = 4 - (2)(2) = 0 $

Next, compute:

$$
E[X^2] = 0.25(1^2 + 1^2 + 3^2 + 3^2) = 0.25(1 + 1 + 9 + 9) = 5
\Rightarrow \operatorname{Var}(X) = 5 - 4 = 1
$$

Similarly:
$$
\operatorname{Var}(Y) = 1
\Rightarrow \sigma_X = \sigma_Y = 1
$$

So:
$$
\rho_{X,Y} = \frac{0}{1 \cdot 1} = 0
$$

âœ… No linear correlation.

---

## ğŸ”¸ Example 2 (Simple Correlated Pair)

Let:
- $ X \in \{1, 2, 3\} $, with $ P(X = x) = \frac{1}{3} $
- $ Y = 2X $

So:

| X | Y | P(X) |
|---|---|------|
| 1 | 2 | 1/3  |
| 2 | 4 | 1/3  |
| 3 | 6 | 1/3  |

Now compute:

- $ E[X] = \frac{1+2+3}{3} = 2 $
- $ E[Y] = \frac{2+4+6}{3} = 4 $
- $ E[XY] = \frac{1 \cdot 2 + 2 \cdot 4 + 3 \cdot 6}{3} = \frac{2 + 8 + 18}{3} = \frac{28}{3} $

Then:

$$
\operatorname{Cov}(X, Y) = \frac{28}{3} - (2)(4) = \frac{28}{3} - 8 = \frac{4}{3}
$$

Now:

- $ E[X^2] = \frac{1^2 + 2^2 + 3^2}{3} = \frac{14}{3} $
- $ \operatorname{Var}(X) = \frac{14}{3} - 4 = \frac{2}{3} $
- $ \operatorname{Var}(Y) = \operatorname{Var}(2X) = 4 \cdot \operatorname{Var}(X) = \frac{8}{3} $

Then:

$$
\rho_{X,Y} = \frac{\frac{4}{3}}{\sqrt{\frac{2}{3}} \cdot \sqrt{\frac{8}{3}}} = \frac{4}{\sqrt{2 \cdot 8}} = \frac{4}{\sqrt{16}} = 1
$$

âœ… Perfect linear correlation.

---

## ğŸ”· 2.6.3 Properties of Correlation

- $ -1 \le \rho_{X,Y} \le 1 $
- $ \rho_{X,Y} = \rho_{Y,X} $
- $ \rho_{X,Y} = 0 $ âŸ¹ **X and Y are uncorrelated**, but not necessarily independent
- **If X and Y are independent**, then $ \rho = 0 $ (but not the reverse!)

---

## âœ… Summary

| Concept         | Formula |
|----------------|---------|
| Correlation Coefficient | $ \rho_{X,Y} = \frac{\operatorname{Cov}(X, Y)}{\sigma_X \cdot \sigma_Y} $ |
| Range | $ -1 \le \rho \le 1 $ |
| Zero correlation | No linear relationship |
| Correlation = 1 or -1 | Perfect linear relationship |

## âœ… **2.7: Properties of Covariance, Variance, and Correlation**

Weâ€™ll go through them category-wise so itâ€™s clean and easy to revise.

---

## ğŸ”· 2.7.1 **Variance Properties**

Let $ X $ be a random variable and $ a, b $ be constants:

### ğŸ”¹ 1. **Variance of a Constant:**
$$
\operatorname{Var}(c) = 0
$$

A constant doesnâ€™t vary, so its variance is 0.

---

### ğŸ”¹ 2. **Scaling:**
$$
\operatorname{Var}(aX) = a^2 \cdot \operatorname{Var}(X)
$$

Multiplying a random variable scales its variance **quadratically** (not linearly!).

---

### ğŸ”¹ 3. **Shift Doesnâ€™t Affect Variance:**
$$
\operatorname{Var}(X + b) = \operatorname{Var}(X)
$$

Adding a constant only shifts the values, doesnâ€™t change the spread.

---

### ğŸ”¹ 4. **Variance of a Sum (General Case):**
$$
\operatorname{Var}(X + Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) + 2\operatorname{Cov}(X, Y)
$$

---

### ğŸ”¹ 5. **If X and Y are Independent:**
$$
\operatorname{Var}(X + Y) = \operatorname{Var}(X) + \operatorname{Var}(Y)
$$

Because:
$$
\operatorname{Cov}(X, Y) = 0 \quad \text{(if independent)}
$$

---

## ğŸ”· 2.7.2 **Covariance Properties**

For constants $ a, b, c, d $, and random variables $ X, Y $:

### ğŸ”¹ 1. **Covariance is Symmetric:**
$$
\operatorname{Cov}(X, Y) = \operatorname{Cov}(Y, X)
$$

---

### ğŸ”¹ 2. **Covariance of Independent Variables:**
$$
X \perp Y \Rightarrow \operatorname{Cov}(X, Y) = 0
$$

But **not the reverse!**

---

### ğŸ”¹ 3. **Cov(X, X) = Var(X)**  
Self-covariance is variance.

---

### ğŸ”¹ 4. **Linearity (Scalar Multiplication):**
$$
\operatorname{Cov}(aX, bY) = ab \cdot \operatorname{Cov}(X, Y)
$$

---

### ğŸ”¹ 5. **Linearity (Addition/Subtraction):**
$$
\operatorname{Cov}(X + Y, Z) = \operatorname{Cov}(X, Z) + \operatorname{Cov}(Y, Z)
$$

In general, Cov is **linear in each argument**.

---

### ğŸ”¹ 6. **Covariance with Constant:**
$$
\operatorname{Cov}(X, c) = 0
$$

Because a constant has no variation.

---

## ğŸ”· 2.7.3 **Correlation Properties**

Let $ \rho = \text{Corr}(X, Y) $:

### ğŸ”¹ 1. **Bounds:**
$$
-1 \leq \rho \leq 1
$$

### ğŸ”¹ 2. **Perfect Correlation:**
- $ \rho = 1 $: $ Y = aX + b $, with $ a > 0 $
- $ \rho = -1 $: $ Y = aX + b $, with $ a < 0 $

Linear relationships only.

---

### ğŸ”¹ 3. **Independence Implies Zero Correlation**  
But **zero correlation â‰  independence** (unless joint distribution is normal or special).

---

### ğŸ”¹ 4. **Invariant to Shifts and Scales:**

$$
\rho_{X,Y} = \rho_{aX+b, cY+d} \quad \text{(if } a, c > 0 \text{)}
$$

This makes correlation very powerful: **unit-less and standardized**.

---

## ğŸ“¦ Example Question (Mixed Concepts)

> Let $ X, Y $ be RVs such that:
> - $ \operatorname{Var}(X) = 4 $, $ \operatorname{Var}(Y) = 9 $
> - $ \operatorname{Cov}(X, Y) = 6 $

### Find:

- $ \operatorname{Var}(X + Y) $
- $ \rho_{X,Y} $

---

**Solution:**

$$
\operatorname{Var}(X+Y) = 4 + 9 + 2(6) = 25
$$

$$
\rho = \frac{\operatorname{Cov}(X,Y)}{\sigma_X \cdot \sigma_Y} = \frac{6}{\sqrt{4} \cdot \sqrt{9}} = \frac{6}{2 \cdot 3} = 1
$$

âœ… Perfectly positively correlated!

---

## âœï¸ Summary Table

| Concept | Property | Formula |
|--------|----------|---------|
| Variance | Scaling | $ \operatorname{Var}(aX) = a^2 \cdot \operatorname{Var}(X) $ |
| Covariance | Symmetry | $ \operatorname{Cov}(X,Y) = \operatorname{Cov}(Y,X) $ |
| Covariance | Linear Combination | $ \operatorname{Cov}(aX+b, cY+d) = ac \cdot \operatorname{Cov}(X, Y) $ |
| Correlation | Standardized | $ \rho = \frac{\operatorname{Cov}(X, Y)}{\sigma_X \sigma_Y} $ |
| Correlation | Range | $ -1 \le \rho \le 1 $ |
| Variance of Sum | General | $ \operatorname{Var}(X+Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) + 2\operatorname{Cov}(X, Y) $ |

## âœ… **2.8: Chebyshevâ€™s Inequality**

Chebyshev's inequality gives us a **lower bound** for the probability that a random variable deviates from its mean.

---

## ğŸ”· 2.8.1 **What is Chebyshevâ€™s Inequality?**

Chebyshevâ€™s inequality states that for any random variable $ X $ with mean $ \mu $ and variance $ \sigma^2 $:

$$
P(|X - \mu| \geq k \sigma) \leq \frac{1}{k^2}
$$

Where:
- $ k $ is any positive number
- $ \mu $ is the mean of $ X $
- $ \sigma $ is the standard deviation of $ X $

In words: **No more than $ \frac{1}{k^2} $ of the distribution lies more than $ k $ standard deviations away from the mean**.

---

## ğŸ”· 2.8.2 **Interpretation**

- The inequality gives us a **worst-case bound** on the probability that a value falls a certain distance away from the mean.
- Itâ€™s **non-parametric**, meaning it works for any probability distribution (even those that are not normal).
- As $ k $ increases, the probability of deviation decreases rapidly (quadratically).

---

## ğŸ”· 2.8.3 **Examples**

### ğŸ”¸ Example 1: Basic Application

Letâ€™s say we have a random variable $ X $ with:
- Mean $ \mu = 5 $
- Standard deviation $ \sigma = 2 $

We want to know the probability that $ X $ deviates by more than 3 units from the mean (i.e., $ |X - 5| \geq 3 $).

From Chebyshevâ€™s inequality:
$$
P(|X - \mu| \geq 3) \leq \frac{1}{\left(\frac{3}{2}\right)^2} = \frac{1}{\left(1.5\right)^2} = \frac{1}{2.25} \approx 0.444
$$

So, the probability that $ X $ deviates more than 3 units from the mean is **at most 44.4%**.

---

### ğŸ”¸ Example 2: Interpretation for Larger Deviations

Now, letâ€™s see what happens when we look for a larger deviation, say $ 5 $ units.

We use $ k = \frac{5}{2} = 2.5 $.

Then:
$$
P(|X - \mu| \geq 5) \leq \frac{1}{(2.5)^2} = \frac{1}{6.25} = 0.16
$$

So, the probability that $ X $ deviates more than 5 units from the mean is **at most 16%**.

---

## ğŸ”· 2.8.4 **Key Takeaways**

- Chebyshevâ€™s inequality is very general and can be applied to any distribution, whether or not itâ€™s normal.
- The inequality provides an **upper bound**, not the exact probability.
- As $ k $ increases, the bound becomes tighter (less probability).
- **The inequality is most useful** when we donâ€™t know the exact form of the distribution (like in real-world situations).

---

### ğŸ”· Summary of Chebyshev's Inequality

For a random variable $ X $ with mean $ \mu $ and variance $ \sigma^2 $, Chebyshev's inequality states:

$$
P(|X - \mu| \geq k \sigma) \leq \frac{1}{k^2}
$$

- The bound decreases quadratically as $ k $ increases.
- The inequality holds for **all** distributions (even non-normal).
- It helps when we need a general estimate for tail probabilities in **non-parametric settings**.

---

### ğŸ“¦ Example Question (Application)

> Given a distribution with $ \mu = 10 $ and $ \sigma = 4 $, use Chebyshevâ€™s inequality to find the probability that the random variable lies within 6 units of the mean.

#### Solution:

Weâ€™re asked for the probability that the value of $ X $ lies within 6 units of the mean:
$$
P(|X - \mu| \geq 6) \Rightarrow k = \frac{6}{4} = 1.5
$$

So:
$$
P(|X - 10| \geq 6) \leq \frac{1}{(1.5)^2} = \frac{1}{2.25} = 0.444
$$

Thus, the probability that $ X $ deviates by **more than 6 units** from the mean is **at most 44.4%**.

## ğŸ“Œ **3.1 Bernoulli Distribution**

The **Bernoulli distribution** is the **simplest discrete probability distribution**, modeling a single experiment with only two outcomes: **success (1)** and **failure (0)**.

---

### ğŸ”¹ Random Variable Definition

Let $ X \sim \text{Bernoulli}(p) $, where:

- $ X = 1 $ with probability $ p $ (success)
- $ X = 0 $ with probability $ 1 - p $ (failure)

---

### ğŸ”¸ 3.1.1 Probability Mass Function (PMF)

$$
P(X = x) = p^x (1 - p)^{1 - x}, \quad \text{for } x \in \{0, 1\}
$$

This compact formula works for both cases:
- If $ x = 1 $: $ P(1) = p $
- If $ x = 0 $: $ P(0) = 1 - p $

---

### ğŸ”¸ 3.1.2 Mean (Expected Value)

$$
\mathbb{E}[X] = 0 \cdot (1 - p) + 1 \cdot p = p
$$

So, the **mean of a Bernoulli random variable is $ p $**.

---

### ğŸ”¸ 3.1.3 Variance

$$
\text{Var}(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2
$$

But for Bernoulli, since $ X^2 = X $ (because $ X \in \{0,1\} $):

$$
\text{Var}(X) = \mathbb{E}[X] - (\mathbb{E}[X])^2 = p - p^2 = p(1 - p)
$$

So, the **variance is $ p(1 - p) $**.

---

### ğŸ”¸ 3.1.4 Moment Generating Function (MGF)

The MGF of a discrete random variable is:

$$
M_X(t) = \mathbb{E}[e^{tX}] = e^{0t}(1 - p) + e^{1t}(p) = (1 - p) + p e^t
$$

---

### ğŸ”¸ 3.1.5 Skewness

$$
\text{Skewness} = \frac{1 - 2p}{\sqrt{p(1 - p)}}
$$

- If $ p = 0.5 $: symmetric (skewness = 0)
- If $ p < 0.5 $: right-skewed
- If $ p > 0.5 $: left-skewed

---

### ğŸ”¸ 3.1.6 Kurtosis

$$
\text{Kurtosis} = \frac{1 - 6p(1 - p)}{p(1 - p)}
$$

This measures **"tailedness"** or how heavy the tails are compared to a normal distribution.

---

### ğŸ§  Trick Question Patterns

1. **If $ \mathbb{E}[X] = 0.3 $, what is $ \text{Var}(X) $?**

$$
\text{Var}(X) = p(1 - p) = 0.3 \cdot 0.7 = 0.21
$$

---

2. **Find skewness when $ \text{Var}(X) = 0.25 $**

If $ p(1 - p) = 0.25 $, then:

$$
p = 0.5 \Rightarrow \text{Skewness} = \frac{1 - 2(0.5)}{\sqrt{0.25}} = \frac{0}{0.5} = 0
$$

---

3. **MGF-based question:**

> Find $ \mathbb{E}[X] $ using MGF:
$$
M_X(t) = (1 - p) + p e^t \Rightarrow M'_X(t) = p e^t
\Rightarrow M'_X(0) = p
$$